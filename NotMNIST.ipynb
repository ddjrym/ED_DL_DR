{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T23:53:34.990017Z",
     "start_time": "2017-12-04T23:51:44.761215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified notMNIST_large.tar.gz\n",
      "Found and verified notMNIST_small.tar.gz\n",
      "Extracting data for notMNIST_large. This may take a while. Please wait.\n",
      "['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']\n",
      "Extracting data for notMNIST_small. This may take a while. Please wait.\n",
      "['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "#SOURCE: noob github\n",
    "\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T23:56:48.710849Z",
     "start_time": "2017-12-04T23:53:35.002102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling notMNIST_large/A.pickle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read: notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file 'notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Could not read: notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file 'notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Could not read: notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file 'notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.128243\n",
      "Standard deviation: 0.443109\n",
      "Pickling notMNIST_large/B.pickle.\n",
      "Could not read: notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file 'notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.00756289\n",
      "Standard deviation: 0.454487\n",
      "Pickling notMNIST_large/C.pickle.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142258\n",
      "Standard deviation: 0.439806\n",
      "Pickling notMNIST_large/D.pickle.\n",
      "Could not read: notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file 'notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0573667\n",
      "Standard deviation: 0.455643\n",
      "Pickling notMNIST_large/E.pickle.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0698991\n",
      "Standard deviation: 0.452942\n",
      "Pickling notMNIST_large/F.pickle.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125583\n",
      "Standard deviation: 0.44709\n",
      "Pickling notMNIST_large/G.pickle.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0945815\n",
      "Standard deviation: 0.44624\n",
      "Pickling notMNIST_large/H.pickle.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0685221\n",
      "Standard deviation: 0.454232\n",
      "Pickling notMNIST_large/I.pickle.\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307862\n",
      "Standard deviation: 0.468899\n",
      "Pickling notMNIST_large/J.pickle.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153358\n",
      "Standard deviation: 0.443656\n",
      "Pickling notMNIST_small/A.pickle.\n",
      "Could not read: notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file 'notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -1.99749e+32\n",
      "Standard deviation: inf\n",
      "Pickling notMNIST_small/B.pickle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:116: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535608\n",
      "Standard deviation: 0.457115\n",
      "Pickling notMNIST_small/C.pickle.\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      "Pickling notMNIST_small/D.pickle.\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      "Pickling notMNIST_small/E.pickle.\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599148\n",
      "Standard deviation: 0.45735\n",
      "Pickling notMNIST_small/F.pickle.\n",
      "Could not read: notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file 'notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.118207\n",
      "Standard deviation: 0.452262\n",
      "Pickling notMNIST_small/G.pickle.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      "Pickling notMNIST_small/H.pickle.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586892\n",
      "Standard deviation: 0.458759\n",
      "Pickling notMNIST_small/I.pickle.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471893\n",
      "Pickling notMNIST_small/J.pickle.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABjklEQVR4nH2RvWqUQRSG33Nmvl1JoVlZkyIIVnaBFK6QKzB6ATZqIkkECwXTaSUiiN6EvwTiFdiILAgi2BtLi4haxMIuO2fOa5GZL/uJeKqZeeY5PzOCTqiacG55dO7MiUHvSgeJCo5f3vlBksaPHRYU8/e+0smcPXHc0TBzd585kRMnbRpqwMpn+kHml9UtegcqwmMyTfjtah9rzNMwYPiWlowv59HDBm0KKhY+MWX3LWhoWhgBQP3Um0ULtI1X0bMgFCcCEDYvFnMQWd2OdpioJgSEjy5acDzYbqwc8qjRC3QaX0MEABrcZqJxDIX4sYdwhr07qIK2NTVfG+Xgcv97z6SM3MKMm5AcPjxDOhS9rRlDXlmCAO+Gs7nc/zmEVPUJJyQn7IZxjGhxBAUYcueDijg4C0WZ4q+I6A8Ai8+fnqxq+LV+3WKBMxRi932TCmzScu22NKYIXs109Agow+WaVuHt6j/xTygtPA1O7QG07w/FJgjFeaTKHQsQALN93Mh0krxUKyiWftNJ560/R1jTWW1KJx8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABtUlEQVR4nG2RPWuUQRSFn7kzr0RFiR+IgsiiBgkICprC/yBpUgiKhYXEwlIQ7PwBYqeNKGkkJFjYCBaWWTXBL0iKBDEKiYWEIBbq7jtzj0USs9nNaR/OmXPP8EBFW+S/lj89ubybAJz8riIp1xvKklzvGxjwRrnL6p5rPSeQcKDE+1N7HEB9g9f6FcVQVQNNZdW6SAIgsGtOpbT1lECgeaFEhfmVSmu0cUjBbXpkyXzN2fWoZkaMCEa3lOHwlRv7S1yPdXuxsNOBau/AYJ9jCsu3xkN3IcKJu6sqObvu/G87TF9KKaVI4NSsikrxkV6n7WDoj1xFUz2F5O008xrHONfbFkRGiJVtoJXGaQzxdhsYGT1SjKCxnkJWccnlqjXRPXyIxmhLrlrzB7fCEI3+MRVXrcUz2CasLAWqq1+U3WvNHid2LJQgDU/Ls2fX+AEiHbHV4O1ZeS65aPU6tnZGU1lFj+59rKVS10X++Chx/cTNz67bWWpNnsdioBPmVivL9ePhWTYiIWGoEGJU+N2ceLYaDN/YKg0cwwz+fv3w8tUSRMrmkOnmvm8/Fz/PvVtogZl3IPgH91JdH3kkRRMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABqElEQVR4nH2Rv2tTURTHP+feG2KQGOogRhS0OBgTpIgOXQTBzaGD4I+1jjo4CS72LxBcFAd1Lo52Utx0FcQuShENogYFS5XYJO/d+3V4WvLqw+9y4H443/M95xrgLKl1en5uttkKPz++/zH6NVm/NQEAZ/Tu9iVJMUqSkoYtDMDRurMppck4qVAa59+aGAQfjz7qxtypRv7h0zCv72zvr1MrTDnyRVlS1Osrh+sA7Ji9uLy2CwMaz5VJUTdrGOa9M4CZAMCiMqWka/hQvJu54P+4PlVU1DOCMSVXlM9KyrWI51+53RjwrgKBGyCgVQ37COhUwyckjHOkKnr8u1KSFghV9L5yRa3t+Ru/pANfFZVrpVGxjeeS4n/oDeVJuV4cwm/H5lgqegfnwW+f7Lk8UZZi1OMO5nzpyhY48VIxz6M2H3YwfIk7GktDpSxLGq0sNDCcD1sf5z0HH2RK2ThJ/Xtn9xaWW7GSulcvzJAyH2Qbq6tvB+tT1g7a118lKY4mkqTRdDDnIczffhMlxfEo2yjFxnwUtd6ZU8f2BRiWIeCJmJq9k3Pd9m+ckt8m3JMv6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABnUlEQVR4nG3SwYuNYRTH8e95n8d0paaE2UxN6qZuZkMSK9ZKWEiJZCEbm5ks/BOyVMzGRhaSxdQkC7PRNESJlMRCI9y7wa1r6n3P8/ws7r3mve79bT/ndDqnw20l1ZJ+fX2xdHkPFgxoflOS5FU/Limrc7dJMGBdrpGUZc7qLRKIZEC2vjElwLbP7NshkRq35hYN1uSqdJIIAGH26idlpVILDPEUjRhjjAXG9LKSktqtol9Pxt3dPWPbupc+FLnwmbND3Equ4s8loODEOEJmlQJj7ySELmDsmoyzCNGZhMZRhPg8AS1NnUOIJxMwcOFQDgo/Ho9j8NZNjMy9j+PnY+dLuVzvpvm/M6bdK4dTyKF3rVuMdobI/rdyucrTBGpoIRgXf8vl2jzfHzPEhhnzy0opV/p+jEAdIwfv9OTJk1abAxvgmbmFZ67snvTnhg2NNbmk9qaUqyopP2wR/u3QR+WqrLL86fHBV9YwV1WWevePQLG1eqQgmVkkv37w6IsVlmonme8oSZ2V6wcChEA98Up6vvHqzfu2yWJOI8ZfuY4mJ6uEmiUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCUlEQVR4nN2SMUoDURCGv3k7JkFiqoCsQki8grUgFjYeQAT7XMAjeAFvYmEj2KdTsLESxU4QsUvAfe+3cNfdjfEC/tVjvvfPDMPPhaJ+K2pGgNGToqRUtKUZAX95HAuwjKYSEXAiIHu966oi5v2tzQ2CHIDoN6drnw2n706IMq4PYyZ7e+iobqpirv7g/NtpDPdYUuHbYblWQwovn4uP1jfLeh3cyxaXJ4PmQllnkO/fV87IvAnh/XlGBQ2zFjSzWEEhtaCAv7f9X9BXVs0sicb5svTDUnmt+nwx1s5efjyZWpmhjIOrOmBhfTjq3mJyHDDyo9bQxALw0Q4GKDWZiQD42TgFVoQa4At9Mnc2Lumm1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nO3QvWrCUBjG8f+bHNMWNJNLEVS8hq4FJ++h0MvpDbRzL6JroXvGQhcnsTgpiHRTMG+eDhpMbDp07zMdzu+8Hxwe5foZV0YE/blcUpHXo4yIsJgNBVhMNQUOBByQrd4vVIq1Or1uh0gBAA9v9619pTK5GeAyXicey9bTRKem2m9ppw+HSqN7y1ny0IvO705IHo7H3VftmcWXCSEcW7zcpdWF4iS9Hn+Ulc62irD5zCjRMKuhmXmJQqqhgN+3/cc/oDXj4euvKBq0P5fLtRw1TXiSS8r13ND5G0I4bxbu0aWBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAByUlEQVR4nGWRTYgNUBTHf+feO897GV+DBYmmpIaaFI3FqyGjxkJZ2mKjrGwosbUgaTbjoyifC9uxslETNUWSKTFJlCxIminGvHc//hYzZt6bd1a3/+/+Op1zDMxlNh6q9/Wu6q4CgCzVX/oMmGP3wx+SpBxjjCnlojiAB4zaaJFyI2qxcpobwENQz1g9Y1T48urd97/VNdu2921xuPkGj9UoSpo4UsPmk+69Fyb24IFjSlLRJTAfQgjBO6Cr5kMwnigr6Q4+LHiw9Pi2Wdif+luf/yfadDYaEG7TkLI+LP3G6PmorKzxivuNIFS0SBV+PaOZItea7hMY63vb1IwPnmncOMXy2qH/g7WUcGOzXsbparIOinv+lOLKrov4Tgj9syoqOkpYCLq4rqaiBnFh8jwZeHAghQ7RjFuKpWjmIPNLajGFO3UvJLR67HiyjsZm7qpyylkjKwi+xQQwz8kZpZQ0uR/z1TaIBfpeqMSoeH8nFW60QvD4E19VGlGNR8OMtkOcY93lOZVGlt5MKbdB8I4dIz9Vmk1JWgYxDxvOvFcpKeWSNLxs0R6qh29+VpGiznXMHAxqg1deT2dNrew4lHkSuK39+4bu/gNspwbufNveHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBUlEQVR4nNWTPU4DMRhEn+2NhIgU0kCXAikNiIKGghPQcAiOgFCoESUXgJsguABXoCBp0IqOhkBDbE8K73o3qxU9U1nzPP5+JHOvIAW9jjAA4LiWl6I+j5l+KMjrFpsYloOlorxusPN3hJjXQSKLJWB4piAC8IVTRfHJ+qGojECMbEoVdNzNBjnJblUjQcMhPbJ9Zq265vevacydHHmR10rn+RpQKiroqLIsFyfbTbcj2skeNck8eNLAbDbUqtjtNrjLx3GoPT3tyTRQLN4KnxOrznO2tfh8qGEk5Gez/lzff4IGi+tcMgzT8FvY6QSD4ZSY4WQIiDN4UJCiynHzHa7SDyn312ltfNn7Nee6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAhklEQVR4nGMIrb706+9/DPDn68FABUYGKSsbXyUGdHB3/dETLxiYWDicj2HqPOkrxMbI8u/f35//MDQy/P7w4R8DEwPD//+YcgwM//4xMDBhk4CBUclRSbgkIyM2CUZGBgYWJiZWDizaGZmY/v9n+SdhZSOOKSnmyXf6JUNYDfbs8P1wkCIABg9daXrLV3EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABKUlEQVR4nNXPsUoDURAF0DtvDAqBKChYRlvBzspSsNDCH7DQ70hlYyEW2vkhtoIWFtopCBYbhEgiRkESSbG7b+ddi3VD3NUP8DavOG+GuUApih2GQNJ4WDYIapfMyIx3i66MrPlnBCDgvl9BAMyfAaooBfIXnMh/x4l6FZQM85A/JxsLEEDwWUWH1TU4QPFSRWJXTUAZ3lSWKraNJI0X5UGpYeWVRgbz6z/RTTlsdGhkSNmCQ3G1iKpgpuVppGU8hhMATlXVAYL6fkQzmqcd5Db9vbK5ddYlLZgPfNqECgBpR71Y5xpLzToQCBEZnZ58qOW9imRJYgzsHS3DFXea92mSpN7IwOH53ixEpWjGosggur2+6guVYVzbx/Ho/a378NjuAHDOxr+BLy7Dm5g4tpl7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SOURCE: noob github\n",
    "\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  \n",
    "  for image_index, image in enumerate(image_files):\n",
    "    image_file = os.path.join(folder, image)\n",
    "    \n",
    "    try:\n",
    "      \n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[image_index, :, :] = image_data\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  num_images = image_index + 1\n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)\n",
    "\n",
    "for folder in train_folders:\n",
    "    image_files = os.listdir(folder)\n",
    "    image_file = os.path.join(folder, image_files[0])\n",
    "    display(Image(filename=image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-05T00:11:32.783455Z",
     "start_time": "2017-12-05T00:11:29.898690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Training: (20000, 28, 28) (20000,)\n",
      "http://localhost:8888/notebooks/1_notmnist.ipynb#Validation: (1000, 28, 28) (1000,)\n",
      "Testing: (1000, 28, 28) (1000,)\n"
     ]
    }
   ],
   "source": [
    "#Source: noog @ github\n",
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):\n",
    "    holder = 0\n",
    "    print (label)\n",
    "    \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        \n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "        \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 20000\n",
    "valid_size = 1000\n",
    "test_size = 1000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('http://localhost:8888/notebooks/1_notmnist.ipynb#Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T23:58:04.351893Z",
     "start_time": "2017-12-04T23:58:04.349617Z"
    }
   },
   "outputs": [],
   "source": [
    "##################### LET'S BEGIN FUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-05T00:19:28.842690Z",
     "start_time": "2017-12-05T00:19:28.838994Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution2D, MaxPooling2D, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-05T00:16:03.701374Z",
     "start_time": "2017-12-05T00:16:03.606015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(filters=64, padding=\"valid\", kernel_size=4, input_shape=(28, 28, 1...)`\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(filters=32, kernel_size=8, padding=\"valid\")`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(filters=8, kernel_size=16, padding=\"valid\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(filters=64, kernel_size=4,\n",
    "                        input_shape=(28, 28, 1), border_mode='valid',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(filters=32, kernel_size=8, border_mode='valid',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(filters=8, kernel_size=16, border_mode='valid',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-05T00:16:13.850224Z",
     "start_time": "2017-12-05T00:16:13.818301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 25, 25, 64)        1088      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 18, 18, 32)        131104    \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 3, 3, 8)           65544     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 3, 3, 8)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 200,178\n",
      "Trainable params: 200,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-05T00:43:36.307110Z",
     "start_time": "2017-12-05T00:32:35.896312Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 129s 6ms/step - loss: 1.4208 - acc: 0.4681 - val_loss: 0.8815 - val_acc: 0.7340\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 135s 7ms/step - loss: 1.3847 - acc: 0.4735 - val_loss: 0.8560 - val_acc: 0.7410\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 127s 6ms/step - loss: 1.3491 - acc: 0.4918 - val_loss: 0.8560 - val_acc: 0.7240\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 134s 7ms/step - loss: 1.3355 - acc: 0.5013 - val_loss: 0.8239 - val_acc: 0.7400\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 133s 7ms/step - loss: 1.3216 - acc: 0.5083 - val_loss: 0.8474 - val_acc: 0.7540\n",
      "Test score: 0.680254273891\n",
      "Test accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "train_data = np.zeros((20000, 28, 28, 1))\n",
    "train_data[:, :, :, 0] = train_dataset\n",
    "test_data = np.zeros((1000, 28, 28, 1))\n",
    "test_data[:, :, :, 0] = test_dataset\n",
    "valid_data = np.zeros((1000, 28, 28, 1))\n",
    "valid_data[:, :, :, 0] = valid_dataset\n",
    "history = model.fit(train_data, np_utils.to_categorical(train_labels, 10),\n",
    "                    batch_size=32, epochs=5,\n",
    "                    verbose=1, validation_data=(valid_data, np_utils.to_categorical(valid_labels, 10)))\n",
    "score = model.evaluate(test_data, np_utils.to_categorical(test_labels, 10), verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
